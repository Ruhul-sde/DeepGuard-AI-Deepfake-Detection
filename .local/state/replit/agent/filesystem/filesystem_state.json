{"file_contents":{"ml_models/image_forensics.py":{"content":"import numpy as np\nimport cv2\nfrom typing import Dict, Any, List\nfrom PIL import Image, ImageFilter\n\nclass ImageForensicsAnalyzer:\n    def __init__(self):\n        self.setup_forensics_tools()\n    \n    def setup_forensics_tools(self):\n        \"\"\"Setup forensic analysis tools\"\"\"\n        print(\"Image forensics analyzer initialized\")\n    \n    async def analyze(self, image: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Perform comprehensive forensic analysis\"\"\"\n        try:\n            # Multiple forensic analyses\n            ela_analysis = self._error_level_analysis(image)\n            noise_analysis = self._noise_consistency_analysis(image)\n            cfa_analysis = self._cfa_artifact_analysis(image)\n            compression_analysis = self._compression_artifact_analysis(image)\n            \n            # Detect editing indicators\n            editing_indicators = self._detect_editing_indicators(\n                ela_analysis, noise_analysis, cfa_analysis, compression_analysis\n            )\n            \n            confidence = self._calculate_forensics_confidence(\n                ela_analysis, noise_analysis, cfa_analysis, compression_analysis\n            )\n            \n            return {\n                \"is_authentic\": len(editing_indicators) == 0,\n                \"editing_indicators\": editing_indicators,\n                \"compression_artifacts\": compression_analysis,\n                \"confidence\": float(confidence),\n                \"detailed_analysis\": {\n                    \"error_level_analysis\": ela_analysis,\n                    \"noise_consistency\": noise_analysis,\n                    \"cfa_artifacts\": cfa_analysis\n                }\n            }\n            \n        except Exception as e:\n            return {\n                \"is_authentic\": True,\n                \"editing_indicators\": [],\n                \"compression_artifacts\": {},\n                \"confidence\": 0.0,\n                \"error\": str(e)\n            }\n    \n    def _error_level_analysis(self, image: np.ndarray) -> Dict[str, float]:\n        \"\"\"Error Level Analysis for JPEG compression artifacts\"\"\"\n        try:\n            # Convert to PIL Image\n            pil_image = Image.fromarray(image)\n            \n            # Save at different quality levels\n            pil_image.save('temp_high.jpg', 'JPEG', quality=95)\n            pil_image.save('temp_low.jpg', 'JPEG', quality=75)\n            \n            # Load and compare\n            high_qual = np.array(Image.open('temp_high.jpg'))\n            low_qual = np.array(Image.open('temp_low.jpg'))\n            \n            # Calculate difference\n            diff = np.abs(high_qual.astype(float) - low_qual.astype(float))\n            ela_score = np.mean(diff) / 255.0\n            \n            import os\n            os.remove('temp_high.jpg')\n            os.remove('temp_low.jpg')\n            \n            return {\"ela_score\": float(ela_score)}\n            \n        except Exception as e:\n            return {\"ela_score\": 0.0, \"error\": str(e)}\n    \n    def _noise_consistency_analysis(self, image: np.ndarray) -> Dict[str, float]:\n        \"\"\"Analyze noise consistency across the image\"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        \n        # Calculate noise pattern using wavelet transform\n        coeffs = self._wavelet_transform(gray)\n        \n        # Analyze noise consistency in different regions\n        regions = [\n            gray[:h//2, :w//2], gray[:h//2, w//2:],\n            gray[h//2:, :w//2], gray[h//2:, w//2:]\n        ]\n        \n        region_noise_levels = [self._estimate_noise_level(region) for region in regions]\n        noise_consistency = np.std(region_noise_levels) / np.mean(region_noise_levels) if np.mean(region_noise_levels) > 0 else 0\n        \n        return {\n            \"noise_consistency\": float(noise_consistency),\n            \"average_noise_level\": float(np.mean(region_noise_levels))\n        }\n    \n    def _wavelet_transform(self, image):\n        \"\"\"Simple wavelet-like transform\"\"\"\n        # Using Laplacian pyramid as approximation\n        level1 = cv2.pyrDown(image)\n        level2 = cv2.pyrDown(level1)\n        return level2\n    \n    def _estimate_noise_level(self, image):\n        \"\"\"Estimate noise level in image region\"\"\"\n        # Using median absolute deviation\n        median = np.median(image)\n        mad = np.median(np.abs(image - median))\n        return float(mad)\n    \n    def _cfa_artifact_analysis(self, image: np.ndarray) -> Dict[str, float]:\n        \"\"\"Analyze Color Filter Array artifacts\"\"\"\n        # CFA interpolation creates specific patterns\n        # Analyze Bayer pattern consistency\n        \n        green_channel = image[:, :, 1]  # Green channel often shows CFA artifacts\n        \n        # Calculate variance in green pixel patterns\n        pattern_variance = np.var([\n            green_channel[::2, ::2],  # Different Bayer pattern positions\n            green_channel[::2, 1::2],\n            green_channel[1::2, ::2],\n            green_channel[1::2, 1::2]\n        ])\n        \n        cfa_score = min(pattern_variance / 1000.0, 1.0)\n        return {\"cfa_artifact_score\": float(cfa_score)}\n    \n    def _compression_artifact_analysis(self, image: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Analyze compression artifacts\"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        \n        # Detect block artifacts (common in JPEG)\n        block_artifacts = self._detect_block_artifacts(gray)\n        \n        # Detect ringing artifacts\n        ringing_artifacts = self._detect_ringing_artifacts(gray)\n        \n        return {\n            \"block_artifacts\": float(block_artifacts),\n            \"ringing_artifacts\": float(ringing_artifacts),\n            \"compression_level\": \"high\" if block_artifacts > 0.7 else \"medium\" if block_artifacts > 0.3 else \"low\"\n        }\n    \n    def _detect_block_artifacts(self, image):\n        \"\"\"Detect JPEG block artifacts\"\"\"\n        # Calculate horizontal and vertical differences at block boundaries\n        h, w = image.shape\n        \n        block_size = 8  # Standard JPEG block size\n        horizontal_artifacts = 0\n        vertical_artifacts = 0\n        \n        for i in range(block_size, h, block_size):\n            row_diff = np.mean(np.abs(image[i, :] - image[i-1, :]))\n            horizontal_artifacts += row_diff\n        \n        for j in range(block_size, w, block_size):\n            col_diff = np.mean(np.abs(image[:, j] - image[:, j-1]))\n            vertical_artifacts += col_diff\n        \n        total_artifacts = (horizontal_artifacts + vertical_artifacts) / (h + w)\n        return float(total_artifacts / 255.0)\n    \n    def _detect_ringing_artifacts(self, image):\n        \"\"\"Detect ringing artifacts around edges\"\"\"\n        # Use Laplacian to find edges\n        edges = cv2.Laplacian(image, cv2.CV_64F)\n        edge_mask = np.abs(edges) > np.mean(np.abs(edges)) * 2\n        \n        # Analyze oscillations near edges\n        ringing_score = 0.0\n        if np.any(edge_mask):\n            # Simple approximation - in real implementation, use more sophisticated methods\n            ringing_score = np.mean(np.abs(edges[edge_mask])) / np.max(np.abs(edges))\n        \n        return float(ringing_score)\n    \n    def _detect_editing_indicators(self, *analyses) -> List[str]:\n        \"\"\"Compile list of editing indicators\"\"\"\n        indicators = []\n        \n        ela_score = analyses[0].get(\"ela_score\", 0)\n        noise_consistency = analyses[1].get(\"noise_consistency\", 0)\n        cfa_score = analyses[2].get(\"cfa_artifact_score\", 0)\n        compression_artifacts = analyses[3]\n        \n        if ela_score > 0.1:\n            indicators.append(\"High error level variation detected\")\n        if noise_consistency > 0.3:\n            indicators.append(\"Inconsistent noise patterns\")\n        if cfa_score > 0.5:\n            indicators.append(\"CFA interpolation artifacts detected\")\n        if compression_artifacts[\"block_artifacts\"] > 0.5:\n            indicators.append(\"Heavy compression artifacts\")\n        \n        return indicators\n    \n    def _calculate_forensics_confidence(self, *analyses) -> float:\n        \"\"\"Calculate confidence in forensic analysis\"\"\"\n        # Base confidence on consistency of indicators\n        scores = [\n            analyses[0].get(\"ela_score\", 0),\n            analyses[1].get(\"noise_consistency\", 0),\n            analyses[2].get(\"cfa_artifact_score\", 0)\n        ]\n        \n        # Higher variance in scores indicates lower confidence\n        variance = np.var(scores)\n        confidence = max(0.0, 1.0 - variance * 3)\n        return float(confidence)","size_bytes":8563},"ml_models/__init__.py":{"content":"","size_bytes":0},"backend/app/api/endpoints/analysis.py":{"content":"from fastapi import APIRouter, File, UploadFile, HTTPException\nfrom fastapi.responses import JSONResponse\nimport magic\nimport os\nfrom typing import Dict, Any\n\nfrom app.services.analysis_service import AnalysisService\nfrom app.core.config import settings\n\nrouter = APIRouter()\nanalysis_service = AnalysisService()\n\n@router.post(\"/analyze-media\")\nasync def analyze_media(file: UploadFile = File(...)):\n    try:\n        # Validate file type\n        content = await file.read()\n        file_type = magic.from_buffer(content, mime=True)\n        \n        if not any(file.filename.lower().endswith(ext) for ext in settings.ALLOWED_EXTENSIONS):\n            raise HTTPException(status_code=400, detail=\"File type not supported\")\n        \n        # Reset file pointer\n        await file.seek(0)\n        \n        # Perform analysis\n        analysis_result = await analysis_service.analyze_media(file, content, file_type)\n        \n        return JSONResponse(content=analysis_result)\n        \n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Analysis failed: {str(e)}\")\n\n@router.get(\"/analysis/{analysis_id}\")\nasync def get_analysis(analysis_id: str):\n    # Implementation for retrieving previous analysis\n    pass","size_bytes":1234},"backend/app/services/analysis_service.py":{"content":"import cv2\nimport numpy as np\nfrom PIL import Image, ImageFilter\nimport io\nimport tempfile\nimport os\nfrom typing import Dict, Any, List\nimport json\nfrom datetime import datetime\n\nfrom ml_models.deepfake_detector import DeepFakeDetector\nfrom ml_models.ai_generated_detector import AIGeneratedDetector\nfrom ml_models.image_forensics import ImageForensicsAnalyzer\nfrom app.utils.metadata_extractor import MetadataExtractor\nfrom app.utils.video_processor import VideoProcessor\n\nclass AnalysisService:\n    def __init__(self):\n        self.deepfake_detector = DeepFakeDetector()\n        self.ai_detector = AIGeneratedDetector()\n        self.forensics_analyzer = ImageForensicsAnalyzer()\n        self.metadata_extractor = MetadataExtractor()\n        self.video_processor = VideoProcessor()\n    \n    async def analyze_media(self, file, content, file_type: str) -> Dict[str, Any]:\n        analysis_id = self._generate_analysis_id()\n        \n        # Extract basic metadata\n        metadata = await self.metadata_extractor.extract(file, content)\n        \n        # Initialize result structure\n        result = {\n            \"analysis_id\": analysis_id,\n            \"filename\": file.filename,\n            \"file_type\": file_type,\n            \"file_size\": len(content),\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"metadata\": metadata,\n            \"authenticity_analysis\": {},\n            \"technical_analysis\": {},\n            \"risk_assessment\": {},\n            \"confidence_scores\": {}\n        }\n        \n        # Perform type-specific analysis\n        if file_type.startswith('image'):\n            analysis_result = await self._analyze_image(content)\n        elif file_type.startswith('video'):\n            analysis_result = await self._analyze_video(file, content)\n        else:\n            raise ValueError(\"Unsupported file type\")\n        \n        result.update(analysis_result)\n        \n        # Generate overall risk assessment\n        result[\"risk_assessment\"] = self._assess_risk(result)\n        \n        return result\n    \n    async def _analyze_image(self, content: bytes) -> Dict[str, Any]:\n        # Convert to PIL Image\n        image = Image.open(io.BytesIO(content))\n        image_np = np.array(image)\n        \n        # Perform various analyses\n        deepfake_analysis = await self.deepfake_detector.analyze_image(image_np)\n        ai_analysis = await self.ai_detector.analyze_image(image_np)\n        forensics_analysis = await self.forensics_analyzer.analyze(image_np)\n        \n        return {\n            \"authenticity_analysis\": {\n                \"is_authentic\": deepfake_analysis.get(\"is_authentic\", False),\n                \"deepfake_probability\": deepfake_analysis.get(\"probability\", 0),\n                \"ai_generated_probability\": ai_analysis.get(\"probability\", 0),\n                \"editing_indicators\": forensics_analysis.get(\"editing_indicators\", []),\n                \"compression_artifacts\": forensics_analysis.get(\"compression_artifacts\", {})\n            },\n            \"technical_analysis\": {\n                \"image_dimensions\": image.size,\n                \"color_mode\": image.mode,\n                \"dpi\": image.info.get('dpi', (72, 72)),\n                \"format\": image.format\n            },\n            \"confidence_scores\": {\n                \"overall_confidence\": self._calculate_overall_confidence(\n                    deepfake_analysis.get(\"probability\", 0),\n                    ai_analysis.get(\"probability\", 0),\n                    forensics_analysis.get(\"confidence\", 0)\n                ),\n                \"deepfake_confidence\": deepfake_analysis.get(\"confidence\", 0),\n                \"ai_generation_confidence\": ai_analysis.get(\"confidence\", 0),\n                \"forensics_confidence\": forensics_analysis.get(\"confidence\", 0)\n            }\n        }\n    \n    async def _analyze_video(self, file, content: bytes) -> Dict[str, Any]:\n        # Save to temporary file for processing\n        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as temp_file:\n            temp_file.write(content)\n            temp_path = temp_file.name\n        \n        try:\n            # Extract frames for analysis\n            frames = await self.video_processor.extract_frames(temp_path)\n            \n            # Analyze multiple frames\n            frame_analyses = []\n            for frame in frames[:10]:  # Analyze first 10 frames\n                frame_analysis = await self._analyze_video_frame(frame)\n                frame_analyses.append(frame_analysis)\n            \n            # Aggregate results\n            aggregated = self._aggregate_video_analysis(frame_analyses)\n            \n            # Additional video-specific analysis\n            video_metadata = await self.video_processor.analyze_video_metadata(temp_path)\n            \n            return {\n                \"authenticity_analysis\": aggregated,\n                \"technical_analysis\": video_metadata,\n                \"confidence_scores\": {\n                    \"overall_confidence\": aggregated.get(\"overall_confidence\", 0),\n                    \"temporal_consistency\": aggregated.get(\"temporal_consistency\", 0)\n                }\n            }\n            \n        finally:\n            # Cleanup\n            if os.path.exists(temp_path):\n                os.unlink(temp_path)\n    \n    def _calculate_overall_confidence(self, *scores):\n        return sum(scores) / len(scores) if scores else 0\n    \n    def _assess_risk(self, analysis_result: Dict) -> Dict[str, Any]:\n        auth_analysis = analysis_result.get(\"authenticity_analysis\", {})\n        confidence = analysis_result.get(\"confidence_scores\", {}).get(\"overall_confidence\", 0)\n        \n        deepfake_prob = auth_analysis.get(\"deepfake_probability\", 0)\n        ai_prob = auth_analysis.get(\"ai_generated_probability\", 0)\n        editing_indicators = auth_analysis.get(\"editing_indicators\", [])\n        \n        risk_score = (deepfake_prob + ai_prob) / 2\n        \n        if risk_score > 0.8 or len(editing_indicators) > 3:\n            risk_level = \"HIGH\"\n        elif risk_score > 0.5 or len(editing_indicators) > 1:\n            risk_level = \"MEDIUM\"\n        else:\n            risk_level = \"LOW\"\n        \n        return {\n            \"risk_level\": risk_level,\n            \"risk_score\": risk_score,\n            \"factors\": [\n                f\"Deepfake probability: {deepfake_prob:.2f}\",\n                f\"AI generation probability: {ai_prob:.2f}\",\n                f\"Editing indicators found: {len(editing_indicators)}\"\n            ]\n        }\n    \n    def _generate_analysis_id(self) -> str:\n        return f\"analysis_{datetime.utcnow().strftime('%Y%m%d_%H%M%S_%f')}\"","size_bytes":6630},"backend/app/__init__.py":{"content":"","size_bytes":0},"backend/app/main.py":{"content":"from fastapi import FastAPI, File, UploadFile, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\nimport uvicorn\nfrom typing import Optional\nimport os\n\nfrom app.api.endpoints import analysis\nfrom app.core.config import settings\n\napp = FastAPI(\n    title=\"Image & Video Authenticity Analyzer\",\n    description=\"Advanced AI-powered media authenticity detection system\",\n    version=\"1.0.0\"\n)\n\n# CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Include routers\napp.include_router(analysis.router, prefix=\"/api/v1\", tags=[\"analysis\"])\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Image & Video Authenticity Analyzer API\"}\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\nif __name__ == \"__main__\":\n    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True)","size_bytes":977},"ml_models/ai_generated_detector.py":{"content":"import numpy as np\nimport cv2\nfrom typing import Dict, Any\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\nclass AIGeneratedDetector:\n    def __init__(self):\n        self.setup_detector()\n    \n    def setup_detector(self):\n        \"\"\"Setup AI generation detection model\"\"\"\n        # In production, load models like CLIP-based detectors or GAN-specific detectors\n        print(\"AI Generation detector initialized\")\n    \n    async def analyze_image(self, image: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Analyze image for AI generation indicators\"\"\"\n        try:\n            # Multiple detection strategies\n            gan_artifacts = self._detect_gan_artifacts(image)\n            frequency_analysis = self._frequency_domain_analysis(image)\n            statistical_analysis = self._statistical_analysis(image)\n            \n            # Combine results\n            ai_probability = self._combine_detection_scores(\n                gan_artifacts, frequency_analysis, statistical_analysis\n            )\n            \n            confidence = self._calculate_detection_confidence(\n                gan_artifacts, frequency_analysis, statistical_analysis\n            )\n            \n            return {\n                \"is_ai_generated\": ai_probability > 0.5,\n                \"probability\": float(ai_probability),\n                \"confidence\": float(confidence),\n                \"detection_methods\": {\n                    \"gan_artifacts\": gan_artifacts,\n                    \"frequency_analysis\": frequency_analysis,\n                    \"statistical_analysis\": statistical_analysis\n                }\n            }\n            \n        except Exception as e:\n            return {\n                \"is_ai_generated\": False,\n                \"probability\": 0.0,\n                \"confidence\": 0.0,\n                \"error\": str(e)\n            }\n    \n    def _detect_gan_artifacts(self, image: np.ndarray) -> float:\n        \"\"\"Detect GAN-specific artifacts\"\"\"\n        # Analyze for common GAN artifacts like:\n        # - Repetitive patterns\n        # - Asymmetric features\n        # - Unnatural textures\n        \n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        \n        # Fourier analysis for repetitive patterns\n        f_transform = np.fft.fft2(gray)\n        f_shift = np.fft.fftshift(f_transform)\n        magnitude_spectrum = np.log(np.abs(f_shift) + 1)\n        \n        # Analyze symmetry in frequency domain\n        height, width = magnitude_spectrum.shape\n        center_y, center_x = height // 2, width // 2\n        \n        # Check quadrant symmetry\n        q1 = magnitude_spectrum[center_y:, center_x:]\n        q2 = magnitude_spectrum[center_y:, :center_x]\n        q3 = magnitude_spectrum[:center_y, :center_x]\n        q4 = magnitude_spectrum[:center_y, center_x:]\n        \n        symmetry_score = (\n            np.abs(np.mean(q1) - np.mean(q2)) +\n            np.abs(np.mean(q3) - np.mean(q4))\n        ) / (2 * np.mean(magnitude_spectrum))\n        \n        artifact_score = min(symmetry_score, 1.0)\n        return float(artifact_score)\n    \n    def _frequency_domain_analysis(self, image: np.ndarray) -> float:\n        \"\"\"Analyze frequency domain characteristics\"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        \n        # Discrete Cosine Transform\n        dct = cv2.dct(np.float32(gray) / 255.0)\n        \n        # Analyze high-frequency components\n        h, w = dct.shape\n        high_freq_energy = np.sum(dct[h//2:, w//2:] ** 2)\n        total_energy = np.sum(dct ** 2)\n        \n        high_freq_ratio = high_freq_energy / total_energy if total_energy > 0 else 0\n        \n        return float(high_freq_ratio)\n    \n    def _statistical_analysis(self, image: np.ndarray) -> float:\n        \"\"\"Perform statistical analysis for AI detection\"\"\"\n        # Analyze color distribution\n        color_std = np.std(image, axis=(0, 1))\n        color_consistency = np.mean(color_std) / 255.0\n        \n        # Analyze local entropy\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        entropy = self._calculate_local_entropy(gray)\n        \n        # AI-generated images often have different entropy distributions\n        entropy_std = np.std(entropy)\n        entropy_score = min(entropy_std / 2.0, 1.0)\n        \n        combined_score = (color_consistency + entropy_score) / 2\n        return float(combined_score)\n    \n    def _calculate_local_entropy(self, image, kernel_size=7):\n        \"\"\"Calculate local entropy\"\"\"\n        from scipy.ndimage import uniform_filter\n        from scipy.stats import entropy\n        \n        # Pad image\n        pad_size = kernel_size // 2\n        padded = np.pad(image, pad_size, mode='reflect')\n        \n        entropy_map = np.zeros_like(image, dtype=np.float32)\n        \n        for i in range(image.shape[0]):\n            for j in range(image.shape[1]):\n                window = padded[i:i+kernel_size, j:j+kernel_size]\n                hist, _ = np.histogram(window, bins=256, range=(0, 256))\n                prob = hist / hist.sum()\n                entropy_map[i, j] = entropy(prob[prob > 0])\n        \n        return entropy_map\n    \n    def _combine_detection_scores(self, *scores) -> float:\n        \"\"\"Combine multiple detection scores\"\"\"\n        return float(np.mean(scores))\n    \n    def _calculate_detection_confidence(self, *scores) -> float:\n        \"\"\"Calculate confidence based on score consistency\"\"\"\n        variance = np.var(scores)\n        confidence = max(0.0, 1.0 - variance * 2)\n        return float(confidence)","size_bytes":5525},"frontend/src/App.js":{"content":"import React, { useState } from 'react';\nimport './App.css';\nimport MediaUpload from './components/MediaUpload';\nimport AnalysisResults from './components/AnalysisResults';\nimport Dashboard from './components/Dashboard';\n\nfunction App() {\n  const [analysisResult, setAnalysisResult] = useState(null);\n  const [loading, setLoading] = useState(false);\n\n  const handleAnalysisComplete = (result) => {\n    setAnalysisResult(result);\n    setLoading(false);\n  };\n\n  const handleAnalysisStart = () => {\n    setLoading(true);\n    setAnalysisResult(null);\n  };\n\n  return (\n    <div className=\"App\">\n      <header className=\"app-header\">\n        <h1>Media Authenticity Analyzer</h1>\n        <p>Advanced AI-powered detection of edited, AI-generated, and deepfake content</p>\n      </header>\n\n      <div className=\"app-container\">\n        <div className=\"main-content\">\n          <MediaUpload \n            onAnalysisStart={handleAnalysisStart}\n            onAnalysisComplete={handleAnalysisComplete}\n          />\n          \n          {loading && (\n            <div className=\"loading-section\">\n              <div className=\"spinner\"></div>\n              <p>Analyzing media content...</p>\n            </div>\n          )}\n\n          {analysisResult && !loading && (\n            <AnalysisResults data={analysisResult} />\n          )}\n        </div>\n\n        <div className=\"sidebar\">\n          <Dashboard />\n        </div>\n      </div>\n    </div>\n  );\n}\n\nexport default App;","size_bytes":1459},"frontend/src/components/MediaUpload.js":{"content":"import React, { useCallback, useState } from 'react';\nimport './MediaUpload.css';\n\nconst MediaUpload = ({ onAnalysisStart, onAnalysisComplete }) => {\n  const [dragActive, setDragActive] = useState(false);\n  const [selectedFile, setSelectedFile] = useState(null);\n\n  const handleDrag = useCallback((e) => {\n    e.preventDefault();\n    e.stopPropagation();\n    if (e.type === \"dragenter\" || e.type === \"dragover\") {\n      setDragActive(true);\n    } else if (e.type === \"dragleave\") {\n      setDragActive(false);\n    }\n  }, []);\n\n  const handleDrop = useCallback((e) => {\n    e.preventDefault();\n    e.stopPropagation();\n    setDragActive(false);\n    \n    const files = e.dataTransfer.files;\n    if (files && files[0]) {\n      handleFileSelection(files[0]);\n    }\n  }, []);\n\n  const handleFileInput = (e) => {\n    const files = e.target.files;\n    if (files && files[0]) {\n      handleFileSelection(files[0]);\n    }\n  };\n\n  const handleFileSelection = (file) => {\n    // Validate file type\n    const allowedTypes = ['image/jpeg', 'image/png', 'image/jpg', 'video/mp4', 'video/avi', 'video/mov'];\n    if (!allowedTypes.includes(file.type)) {\n      alert('Please select a valid image or video file');\n      return;\n    }\n\n    // Validate file size (100MB max)\n    if (file.size > 100 * 1024 * 1024) {\n      alert('File size must be less than 100MB');\n      return;\n    }\n\n    setSelectedFile(file);\n  };\n\n  const analyzeMedia = async () => {\n    if (!selectedFile) return;\n\n    onAnalysisStart();\n    \n    const formData = new FormData();\n    formData.append('file', selectedFile);\n\n    try {\n      const response = await fetch('http://localhost:8000/api/v1/analyze-media', {\n        method: 'POST',\n        body: formData,\n      });\n\n      if (!response.ok) {\n        throw new Error('Analysis failed');\n      }\n\n      const result = await response.json();\n      onAnalysisComplete(result);\n    } catch (error) {\n      console.error('Error analyzing media:', error);\n      alert('Analysis failed. Please try again.');\n      onAnalysisComplete(null);\n    }\n  };\n\n  return (\n    <div className=\"media-upload\">\n      <div \n        className={`upload-area ${dragActive ? 'drag-active' : ''}`}\n        onDragEnter={handleDrag}\n        onDragLeave={handleDrag}\n        onDragOver={handleDrag}\n        onDrop={handleDrop}\n      >\n        <div className=\"upload-content\">\n          <div className=\"upload-icon\">üìÅ</div>\n          <h3>Drag & Drop your media file</h3>\n          <p>Supports: JPG, PNG, MP4, AVI, MOV (Max 100MB)</p>\n          <input\n            type=\"file\"\n            id=\"file-input\"\n            onChange={handleFileInput}\n            accept=\".jpg,.jpeg,.png,.mp4,.avi,.mov\"\n            style={{ display: 'none' }}\n          />\n          <button \n            className=\"browse-btn\"\n            onClick={() => document.getElementById('file-input').click()}\n          >\n            Browse Files\n          </button>\n        </div>\n      </div>\n\n      {selectedFile && (\n        <div className=\"file-info\">\n          <div className=\"file-details\">\n            <span className=\"file-name\">{selectedFile.name}</span>\n            <span className=\"file-size\">\n              {(selectedFile.size / (1024 * 1024)).toFixed(2)} MB\n            </span>\n          </div>\n          <button className=\"analyze-btn\" onClick={analyzeMedia}>\n            Analyze Authenticity\n          </button>\n        </div>\n      )}\n    </div>\n  );\n};\n\nexport default MediaUpload;","size_bytes":3450},"README.md":{"content":"# üß† Image & Video Authenticity Analyzer\n\nA full-stack project for detecting deepfakes and AI-generated media using machine learning and forensic analysis.\n\n## Structure\n```\nimage-video-authenticity-analyzer/\n‚îú‚îÄ‚îÄ backend/           # FastAPI backend\n‚îú‚îÄ‚îÄ frontend/          # React/Vite frontend\n‚îú‚îÄ‚îÄ ml_models/         # ML model scripts for authenticity detection\n‚îú‚îÄ‚îÄ docker-compose.yml # Combined service orchestration\n‚îî‚îÄ‚îÄ README.md          # Project documentation\n```\n\n## üöÄ Quick Start\n```bash\n# Create environment and install backend deps\ncd backend\npip install -r requirements.txt\n\n# Run backend\nuvicorn app.main:app --reload\n\n# Frontend setup\ncd ../frontend\nnpm install\nnpm run dev\n```\n","size_bytes":727},"ml_models/deepfake_detector.py":{"content":"import tensorflow as tf\nimport numpy as np\nimport cv2\nfrom typing import Dict, Any\nimport os\n\nclass DeepFakeDetector:\n    def __init__(self, model_path: str = None):\n        self.model = None\n        self.input_size = (256, 256)\n        self.load_model(model_path)\n    \n    def load_model(self, model_path: str):\n        \"\"\"Load pre-trained deepfake detection model\"\"\"\n        try:\n            # In production, load actual trained model\n            # self.model = tf.keras.models.load_model(model_path)\n            print(\"Deepfake detection model loaded\")\n        except Exception as e:\n            print(f\"Model loading failed: {e}\")\n            self._create_dummy_model()\n    \n    def _create_dummy_model(self):\n        \"\"\"Create dummy model for demonstration\"\"\"\n        self.model = tf.keras.Sequential([\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(1, activation='sigmoid')\n        ])\n    \n    async def analyze_image(self, image: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Analyze image for deepfake indicators\"\"\"\n        try:\n            # Preprocess image\n            processed = self._preprocess_image(image)\n            \n            # Extract features for analysis\n            features = self._extract_deepfake_features(image)\n            \n            # Make prediction (using dummy logic for demonstration)\n            prediction = self._predict_deepfake(features)\n            confidence = self._calculate_confidence(features)\n            \n            return {\n                \"is_authentic\": prediction < 0.5,\n                \"probability\": float(prediction),\n                \"confidence\": float(confidence),\n                \"features_analyzed\": list(features.keys()),\n                \"analysis_method\": \"CNN-based deepfake detection\"\n            }\n            \n        except Exception as e:\n            return {\n                \"is_authentic\": True,\n                \"probability\": 0.0,\n                \"confidence\": 0.0,\n                \"error\": str(e)\n            }\n    \n    def _preprocess_image(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"Preprocess image for model input\"\"\"\n        # Resize\n        image = cv2.resize(image, self.input_size)\n        # Normalize\n        image = image.astype(np.float32) / 255.0\n        # Expand dimensions for batch\n        image = np.expand_dims(image, axis=0)\n        return image\n    \n    def _extract_deepfake_features(self, image: np.ndarray) -> Dict[str, float]:\n        \"\"\"Extract features indicative of deepfakes\"\"\"\n        # Analyze facial features consistency\n        face_consistency = self._analyze_facial_consistency(image)\n        \n        # Analyze blending artifacts\n        blending_artifacts = self._detect_blending_artifacts(image)\n        \n        # Analyze color consistency\n        color_consistency = self._analyze_color_consistency(image)\n        \n        # Analyze texture patterns\n        texture_analysis = self._analyze_texture_patterns(image)\n        \n        return {\n            \"face_consistency\": face_consistency,\n            \"blending_artifacts\": blending_artifacts,\n            \"color_consistency\": color_consistency,\n            \"texture_anomalies\": texture_analysis\n        }\n    \n    def _analyze_facial_consistency(self, image: np.ndarray) -> float:\n        \"\"\"Analyze consistency in facial features\"\"\"\n        # Implementation using facial landmarks and symmetry analysis\n        try:\n            # Convert to grayscale\n            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n            \n            # Simple edge-based consistency measure\n            edges = cv2.Canny(gray, 100, 200)\n            edge_consistency = np.mean(edges) / 255.0\n            \n            return float(edge_consistency)\n        except:\n            return 0.5\n    \n    def _detect_blending_artifacts(self, image: np.ndarray) -> float:\n        \"\"\"Detect image blending artifacts\"\"\"\n        # Analyze high-frequency components\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n        \n        # Normalize to 0-1 range\n        artifact_score = min(laplacian_var / 1000.0, 1.0)\n        return float(artifact_score)\n    \n    def _analyze_color_consistency(self, image: np.ndarray) -> float:\n        \"\"\"Analyze color consistency across the image\"\"\"\n        # Calculate color variance across channels\n        channel_variances = [np.var(image[:, :, i]) for i in range(3)]\n        avg_variance = np.mean(channel_variances)\n        \n        # Normalize\n        consistency = 1.0 - min(avg_variance / 10000.0, 1.0)\n        return float(consistency)\n    \n    def _analyze_texture_patterns(self, image: np.ndarray) -> float:\n        \"\"\"Analyze texture patterns for anomalies\"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        \n        # Calculate LBP (Local Binary Patterns) variance\n        lbp = self._local_binary_pattern(gray)\n        lbp_variance = np.var(lbp)\n        \n        anomaly_score = min(lbp_variance / 1000.0, 1.0)\n        return float(anomaly_score)\n    \n    def _local_binary_pattern(self, image, points=8, radius=1):\n        \"\"\"Calculate Local Binary Pattern\"\"\"\n        lbp = np.zeros_like(image)\n        for i in range(radius, image.shape[0]-radius):\n            for j in range(radius, image.shape[1]-radius):\n                center = image[i, j]\n                binary = ''\n                for p in range(points):\n                    x = i + radius * np.cos(2 * np.pi * p / points)\n                    y = j - radius * np.sin(2 * np.pi * p / points)\n                    x, y = int(x), int(y)\n                    binary += '1' if image[x, y] >= center else '0'\n                lbp[i, j] = int(binary, 2)\n        return lbp\n    \n    def _predict_deepfake(self, features: Dict[str, float]) -> float:\n        \"\"\"Make deepfake prediction based on features\"\"\"\n        # Weighted combination of feature scores\n        weights = {\n            'face_consistency': 0.3,\n            'blending_artifacts': 0.3,\n            'color_consistency': 0.2,\n            'texture_anomalies': 0.2\n        }\n        \n        score = sum(features[key] * weights[key] for key in weights)\n        return float(score)\n    \n    def _calculate_confidence(self, features: Dict[str, float]) -> float:\n        \"\"\"Calculate confidence score for the prediction\"\"\"\n        feature_variance = np.var(list(features.values()))\n        confidence = max(0.0, 1.0 - feature_variance)\n        return float(confidence)","size_bytes":6485},"backend/app/core/config.py":{"content":"import os\nfrom pydantic_settings import BaseSettings\n\nclass Settings(BaseSettings):\n    PROJECT_NAME: str = \"Media Authenticity Analyzer\"\n    VERSION: str = \"1.0.0\"\n    API_V1_STR: str = \"/api/v1\"\n    \n    # Model paths\n    DEEPTRACE_MODEL_PATH: str = os.getenv(\"DEEPTRACE_MODEL_PATH\", \"./models/deeptrace.h5\")\n    MESONET_MODEL_PATH: str = os.getenv(\"MESONET_MODEL_PATH\", \"./models/mesonet.h5\")\n    FORENSICS_MODEL_PATH: str = os.getenv(\"FORENSICS_MODEL_PATH\", \"./models/forensics.pth\")\n    \n    # Analysis thresholds\n    DEEPFAKE_THRESHOLD: float = 0.7\n    AI_GENERATED_THRESHOLD: float = 0.6\n    EDITING_THRESHOLD: float = 0.5\n    \n    # File settings\n    MAX_FILE_SIZE: int = 100 * 1024 * 1024  # 100MB\n    ALLOWED_EXTENSIONS: list = [\".jpg\", \".jpeg\", \".png\", \".mp4\", \".avi\", \".mov\"]\n    \n    class Config:\n        case_sensitive = True\n\nsettings = Settings()","size_bytes":863}},"version":2}